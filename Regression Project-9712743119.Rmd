---
title: "Project1"
author: "Mina Kanaani"
date: "`r Sys.Date()`"
output: word_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo =TRUE)
```

### Abstract

### Regression Project

This is the Report on the first R and Statistical Analysis's project on regression and machine learning.

In this report I aim to predict the health charges of Bangladeshi citizens based on some important features such as their age,sex, BMI, number of children they have, etc.

Based on their characteristics, I built three models: Linear Regression ,KNN model and Regression Tree model.

At the end, I tried out some tests to examine the prediction which showed us good results.

### Why insurance cost?

Rising health care costs are a major economic and public health issue worldwide : According to the World Health Organization, health care accounted for 7.9% of Europe's gross domestic product (GDP) in 2015. In Switzerland, the health care sector contributes substantially to the national GDP, and has increased from 10.7 to 12.1% between 2010 and 2015. Moreover, because health care utilization costs may serve as a surrogate for an individual's health status, **understanding which factors contribute to increases** in health expenditures may provide insight into risk factors and potential starting points for preventive measures.

### Flowchart

```{r}
library(DiagrammeR)

DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = TB, label = 'Project Flowchart',labelloc= ta]
node [shape = rectangle, style = filled, fillcolor = lightblue]

'Calling libraries' -> 'EDA analysis' -> 'Processing data' -> 'Linear model'
'Processing data' -> 'KNN model'
'Processing data' -> 'Regression Tree model'
'Linear model'-> Visualization
'KNN model'-> Visualization
'Regression Tree model'-> Visualization
Visualization->'RMSE candid model'
}")
```

First we recall the libraries we need in this project.

```{r}
library(readxl)
library(tibble)   # data frame printing
library(dplyr)      # data manipulation
library(caret)     # fitting knn
library(rpart)    # fitting trees
library(rpart.plot) # plotting trees
library(knitr)
library(Hmisc)   #using for "describe" function
library(cowplot)
#EDA needed library
library(ggplot2)
```

Then we read the excel file attached to this file to be able to access the data .

```{r}
#reading the file
insu_data=read_excel("insurance.xlsx")
kable(head(insu_data,5))
```

## EDA Visualization

In this part, we use EDA analysis tools to visualize our features and their binary correlation with charges to determine which features have significant effect and which are not of much importance.

First , we check if our data has any missing values and we use "describe" function to take an overall look at the observations and attributes.

As it is shown, there is no missing values in any features, so we can go straight to drawing plot and analysis.

```{r}
describe(insu_data)
sum(is.na(insu_data))
```

In this part, First I used the simple "plot" function to draw charts of correlation between charges and age , and then charges and bmi, for both bmi and age are quantitative valuables.

### First plot

```{r}
par(mfrow=c(1,2))
plot1_1=plot(insu_data$charges~insu_data$age,col='light green',pch=20,cex=1, main="Correlation : Charges and Age")+
  geom_jitter()
grid()
plot1_2=plot(insu_data$charges~insu_data$bmi,col='purple',pch=20,cex=1, main="Correlation : Charges and bmi")+
  geom_jitter()
grid()
par(mfrow=c(1,1))
```

As we can see int the result above, as the age and bmi increase, the charges increase too. so there is a increasing trend here.

In the next plots, I try to plot charts for relationship between charges and sex and number of children. As it is known,the "children" and "sex" features are both qualitative variables so I can not simply use "plot" function to draw it, for this part and next one, I use "ggplot" from package ggplot2 and with the help of jittering I reduce the amount of overlapping to make a more accurate plots.

I use"plot_grid" to combine to two plots together to be able to compare them in a better way.

### Second plot

```{r}
plot2_1=ggplot(insu_data, aes(sex, charges)) +
  geom_jitter(aes(color = sex), alpha = 0.7) +
  theme_light()
plot2_2=ggplot(insu_data, aes(children, charges)) +
  geom_jitter(aes(color = children), alpha = 0.7) +
  theme_light()

plot2_3=plot_grid(plot2_1,plot2_2)
title2=ggdraw()+draw_label("Correlation between Charges and children/Sex")
plot2=plot_grid(title2,plot2_3,ncol=1,rel_heights=c(0.1, 1))
plot2
```

As illustrated, gender does not apparently have any effect on the charges, with number of children, we can see a decreasing trend, the more children, the less the charges which is odd and doesn't seem logical.

In the third section , I used the same methods with ggplot and jittering to plot the charts illustrating relationship between charges with "region" and "smoker" features since they are both Qualitative.

### Third plot

```{r}
plot3_1=ggplot(insu_data,aes(region,charges))+
  geom_jitter(aes(col=region),alpha=0.7)+
  theme_light()

plot3_2=ggplot(insu_data,aes(smoker,charges))+
  geom_jitter(aes(col=smoker),alpha=0.7)+
  theme_light()
plot3_3=plot_grid(plot3_1,plot3_2)
title3=ggdraw()+draw_label("Correlation between Charges and Region/Smoker")
plot3=plot_grid(title3,plot3_3,ncol=1,rel_heights=c(0.1, 1))
plot3
```

As it is illustrated, in the result above, no obvious connection can be seen between region and charges, also as expected, charges for smokers are higher than for non_smokers.

## processing data

In this part , we start of with splitting the "insu_data" into "train" and "test" . before doing so, since the analysis above illustrated , apparently "Region" and "sex" have no effect on charges so I used "subset" function to eliminated them from my main data before splitting it.

```{r}
insu_data=subset(insu_data,select = -c(sex,region))
```

Now we start the splitting, 80% of data in train and 20% in test . I set the set.seed(42) to be able to work with constant generated numbers.

### Split Train and test

```{r}

set.seed(42)
insu_trn_idx = sample(nrow(insu_data), size = 0.8 * nrow(insu_data))
insu_trn = insu_data[insu_trn_idx, ]
insu_tst = insu_data[-insu_trn_idx, ]
```

Now it's time to split the train data to "estimation" and "validation" , same percentage of splitting is applicable here.

### Split estimation and validation

```{r}

insu_est_idx = sample(nrow(insu_trn), size = 0.8 * nrow(insu_trn))
insu_est = insu_trn[insu_est_idx, ]
insu_val = insu_trn[-insu_est_idx, ]
```

Here we use "skim" and "str" functions to look at the overall "train" data .

```{r}
#check data
head(insu_trn, n = 5)
#skimr::skim(insu_trn)
str(insu_trn)
#View(insu_trn)
```

Then using GGally::ggpairs , I illustrated the correlations and relationships between the 4 main features.

```{r}
GGally::ggpairs(insu_trn, progress = FALSE)
```

As it is shown in the plot above, there is considerable correlation between charges with bmi and age, what we expected from the EDA visualization , we can see that the more the number of children the less is the charge which is not a logical appearance and as we expected , the smokers have more charges for insurance than non smokers.

## Linear models

In the first part of Regression modeling and fitting models to predict the charges , I used the linear modeling , using "lm" function to fit the models.The 5 models I considered for this part are consist of one that includes none of variable,one that includes 2 main quantitative variables(bmi,age) ,the other includes 2 main quantitative with one qualitative(smoker), the one with all of the features , and lastly , the one consists of all the variable with the binary interaction between them.

### Fitting models

```{r}
insu_mod_list=list(
  insu_mod_1=lm(charges~1,data=insu_est),
  insu_mod_2=lm(charges~age+bmi,data=insu_est),
  insu_mod_3=lm(charges~age+bmi+smoker,data=insu_est),
  insu_mod_4=lm(charges~.,data=insu_est),
  insu_mod_5=step(lm(charges ~ . ^ 2, data = insu_est), trace = FALSE),
  insu_mod_6=lm(charges~poly(bmi,degree=2),data=insu_est),
  insu_mod_7=lm(charges~poly(age,degree=2),data=insu_est)
  
)
insu_mod_5=insu_mod_list$insu_mod_5
```

Then , I write down the code for RMSE calculator , a function which is used in all the other sections of this project.

### RMSE Function

```{r}

calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

Now using lapply and sapply we predict based on Validation data using all the models in the list above. Then using RMSE calculator written above, I compare RMSE to find the best model with least RMSE.

I use the "cbind" and "barplot" to illustrate the trends in RMSEs both in validation and estimation data.

### Select candid model

```{r}

insu_mod_val_pred = lapply(insu_mod_list, predict, insu_val )
val_RMSE=sapply(insu_mod_val_pred, calc_rmse, actual = insu_val$charges) 
barplot(val_RMSE,col='pink',beside=T,legend.text = T)

```

As we compare the validation RMSE, we can see the 5th model consist of all variables with their binary interactions with each other is the candid model with least RMSE.

So to conclude, the best model in this part is the "insu_mod_5" where we included all the features with their binary interaction. so we illustrate the real data with predicted using plot to see the credibility of our model. later on , we compare the other models from other methods with this one.

### Visualization

```{r}
plot(
  x = insu_val$charges,
  y = predict(insu_mod_5, insu_val),
  pch = 20, col = "darkgreen",
  main = "Credit: Predicted vs Actual, Test Data",
  xlab = "Actual",
  ylab = "Predicted"
)
abline(a = 0, b = 1, lwd = 2)
grid()
```

As it is shown, the model mostly is moving closely to the line y=x in which x is the real "charges" and the green plot are the predicted one which at most places are near accurate.

## KNN( K-Nearest Neighbor)

In the second method for creating suitable model to fit the real data and predict accordingly, I use the KNN model, in which we assume the that similar data exist in close proximity. In other words, similar data are near to each other. Before we start with fitting candidate models of KNN we have to use Scale function to normalize the quantifiable features such as age and bmi.

### Normalization

```{r}
#age
insu_est$age.s=scale(insu_est$age)
age.center = attr(insu_est$age.s,"scaled:center")
age.scale = attr(insu_est$age.s,"scaled:scale")
#bmi
insu_est$bmi.s=scale(insu_est$bmi)
bmi.center = attr(insu_est$bmi.s,"scaled:center")
bmi.scale = attr(insu_est$bmi.s,"scaled:scale")

# normalization of validation data
#age
insu_val$age.s = scale(insu_val$age, center = age.center, scale = age.scale)
#bmi
insu_val$bmi.s = scale(insu_val$bmi, center = bmi.center, scale = bmi.scale)

```

now after we normalized our features, I created a function to generate K ranging from 1 to 100 and after comparing the RMSEs,we fit the final model.

### Fitting models

```{r}
knn_mod_list=list()
creat_knn_normal=function(i){
  knnreg(charges ~ age.s +smoker + bmi.s+children, data = insu_est, k =i)
}
for(i in 1:100){
  knn_mod_list[i]=list(creat_knn_normal(i))
}
#length(knn_mod_list)
```

Now using lapply and sapply we predict based on Validation data using all the models in the list above. Then using RMSE calculator written above, I compare RMSE to find the best model with least RMSE.

### Select candid model

```{r}
knn_val_pred=lapply(knn_mod_list,predict,insu_val)
knn_val_rmse=sapply(knn_val_pred, calc_rmse, insu_val$charges)
opt_mode=which.min(knn_val_rmse)
insu_KNN_06=knn_mod_list[[opt_mode]]
insu_KNN_06
```

The model with k=6 is the best one with lowest RMSE.

```{r}
knn_table_normal=data.frame(knn_val_rmse)
knn_table_normal$K=c(1:100)
knn_table_normal=knn_table_normal%>%select(-knn_val_rmse,everything())
head(knn_table_normal,7)
```

Now for comparison and better analysis, we fit the models without normalization first to see the difference. We repeat all the process above with not normalized data.

### No Normalization

```{r}
#not normal
knn_mod_list_not=list()
creat_knn_not=function(i){
  knnreg(charges ~ age +smoker + bmi+children, data = insu_est, k =i)
}
for(i in 1:100){
  knn_mod_list_not[i]=list(creat_knn_not(i))
}
length(knn_mod_list_not)

#fitting candid models
knn_val_pred_not=lapply(knn_mod_list_not,predict,insu_val)
# model selection
knn_val_rmse_not=sapply(knn_val_pred_not, calc_rmse, insu_val$charges)
opt_mode_not=which.min(knn_val_rmse_not)
insu_KNN_02=knn_mod_list_not[[opt_mode_not]]
insu_KNN_02
```

The model with k=2 is the best one with lowest RMSE in the not normalized features.

```{r}
knn_table_not=data.frame(knn_val_rmse_not)
knn_table_not$K=c(1:100)
knn_table_not=knn_table_not%>%select(-knn_val_rmse_not,everything())
head(knn_table_not,5)
```

Now we use visualization tools to be able to compare the two normalized and not normalized models in a better way.

### Visualization

```{r}

par(mfrow=c(1,1))
plot(knn_table_normal,xlab="K",ylab="RMSE",main="RMSE  normal", pch = 20,cex=1,col = "dodgerblue",ylim=c(6000,13000))
points(knn_table_not,xlab="K",ylab="RMSE",main="RMSE not normal", pch = 20,cex=1,col = "red")
legend(60,8000,legend = c('normalized data','nor normalized data'),col=c('dodgerblue','red'),cex=1,lty=1)
```

As it is illustrated in the plot above, we can see that not normalized RMSEs are higher than normalized one in any K so we can conclude that normalizing quantifiable features have signifiacant effect on RMSE and therefore gradual candid model the best model from this method there for is 6-nearest neighbor with K=6.

So we illustrate the real data with predicted using plot to see the credibility of our model. later on , we compare the other models from other methods with this one

```{r}
#visualization
plot(
  x = insu_val$charges,
  y = predict(knn_mod_list[[6]], insu_val),
  pch = 20, col = "blue",
  main = "Credit: Predicted vs Actual, Test Data",
  xlab = "Actual",
  ylab = "Predicted"
)
abline(a = 0, b = 1, lwd = 2)
grid()
```

As it is shown, the model mostly is moving closely to the line y=x in which x is the real "charges" and the blue plot are the predicted one which is not as accurate as the last linear model but at most places is following the real charge.

## Regression Tree

In the last method I use to fit suitable models for predicting insurance cost, I use the Regression Tree to split the data based on important features and to achieve a final node. With regression tree ,I create a list with 10 models, 5 consist of minsplit =5 and cp ranging from 0 to 1.

### Fitting models

```{r}
tree_mod_list = list(
  insu_tree_1 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.000,minsplit=5),
  insu_tree_2 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.001,minsplit=5),
  insu_tree_3 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.01,minsplit=5),
  insu_tree_4 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.1,minsplit=5),
  insu_tree_5 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 1,minsplit=5),
  insu_tree_6 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.000,minsplit=20),
  insu_tree_7 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.001,minsplit=20),
  insu_tree_8 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.01,minsplit=20),
  insu_tree_9 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 0.1,minsplit=20),
  insu_tree_10 = rpart(charges ~ age +smoker + bmi+children, data = insu_est, cp = 1,minsplit=20)
)

insu_tree_7=tree_mod_list$insu_tree_7
```

As usual , using lapply and sapply we create prediction based on each model and then using the RMSE calculator function we can compare the models and choose the best one.

### Select candid model

```{r}
tree_val_predict=lapply(tree_mod_list,predict,insu_val)
tree_val_rmse=sapply(tree_val_predict,calc_rmse,insu_val$charges)
opt_tree_mod=tree_mod_list[which.min(tree_val_rmse)]
which.min(tree_val_rmse)
```

The tree model 7 with cp = 0.001, minsplit=20 is the candid model with lowest RMSE.

Now we use Kable to show each model with different attributes of cp and minsplit along with their RMSE.

```{r}
tree_table=data.frame(tree_val_rmse)
tree_table$minsplit=c(rep(20,5),rep(5,5))
tree_table$cp=c(rep(c(0,0.001,0.01,0.1,1),1))
kable(tree_table)
```

Now using package "rpart", we draw regression trees to analysis on the data given.

### Visualization

```{r}
insu_tree = rpart(charges ~ age +smoker + bmi+children, data = insu_est ,cp=0.001,minsplit=20)
#insu_tree
rpart.plot(insu_tree)
```

Analysis: as we can see in the plot, most frequent cut offs are based on aget so we can say it's our most important variables, following there are several splits with bmi variable which indicates that these two quantity variables are important. there is one first split on smoker which as we expect should have serious effect on insurance costs and splits on children which we can conclude that all 4 features are effective.

For example, from the first split, we check if the person is a smoker or not,if they are smokers, we go to the left side of the chart, now if the person age is lower that 43 ,we go to the lest part and we check the number of children they have, if it is more than 1, we go to right side and check again if the number of children is more than 2, this part we assume that the number is lower than 2, so we go to left ,there is another split on age to check whether they are under 20 or not , if they are older than 20 years old , we can predict that the insurance charge is 6191 and only 10% of all the reported data have the same conditions.

Now as the same as the previous method, we visualize our candid method to see the trend of predicted data with actual ones.

```{r}
plot(
  x = insu_val$charges,
  y = predict(insu_tree_7, insu_val),
  pch = 20, col = "purple",
  main = "Credit: Predicted vs Actual, Test Data",
  xlab = "Actual",
  ylab = "Predicted"
)
abline(a = 0, b = 1, lwd = 2)
grid()
```

As it is shown, the model mostly is moving closely to the line y=x in which x is the real "charges" and the purple plot are the predicted one which is even more accurate than the last two models and at most places is following the real charge.

## Visualization of 3 models

In the last section, we put together the tree candid models from Linear, KNN, Regression Tree method and then by calculating RMSEs we choose our final optimal model.

### Combination

```{r}
par(mfrow = c(1, 3))
#linear
plot(
  x = insu_val$charges,
  y = predict(insu_mod_5, insu_val),
  pch = 20, col = "darkgreen",
  main = "Credit: Predicted vs Actual, Linear",
  xlab = "Actual",
  ylab = "Predicted"
)
abline(a = 0, b = 1, lwd = 2)
grid()
#KNN
plot(
  x = insu_val$charges,
  y = predict(knn_mod_list[[6]], insu_val),
  pch = 20, col = "blue",
  main = "Credit: Predicted vs Actual, KNN",
  xlab = "Actual",
  ylab = "Predicted"
)
abline(a = 0, b = 1, lwd = 2)
grid()
#TREE
plot(
  x = insu_val$charges,
  y = predict(insu_tree_7, insu_val),
  pch = 20, col = "purple",
  main = "Credit: Predicted vs Actual, REG Tree",
  xlab = "Actual",
  ylab = "Predicted"
)
abline(a = 0, b = 1, lwd = 2)
grid()

```

From the illustrated data, we can assume that the KNN model must not be the optimal because of the frequent discrepancies in the predicted data. between Linear and Regression tree we have to check the RMSEs.

### Select final model

```{r}
optimal_models=list(insu_mod_5,knn_mod_list[[6]],insu_tree_7)
optimal_val_pred=lapply(optimal_models,predict,insu_val)
RMSE_all=sapply(optimal_val_pred,calc_rmse,actual=insu_val$charges)
RMSE_all
```

The best model out of three is the linear model "insu_mod_5" which consist of all the features with their binary interactions. as we can see the two models of regression tree and linear have close RMSE and both seem to be good models.

At the end we fit the candid model on the test data to calculate the RMSE.

### Final RMSE

```{r}
final_model=step(lm(charges ~ . ^ 2, data = insu_trn), trace = FALSE)
predict_insu=predict(final_model,insu_tst)
calc_rmse(insu_tst$charges,predict_insu)
# the RSME is 5041.705
par(mfrow=c(1,1))
```
